\setcounter{chapter}{1}
\chapter{Matrix Lie Groups Solutions}

\begin{exercise}[1]
  Let $a$ be an irrational real number. Show that the set of numbers of the form
  $e^{2\pi ina}$, $n \in \Z$, is dense in $S^1$. Now, let $G$ be the following
  subgroup of $\GL(2, \C)$
  \[%
    G = \left\{\left.
      \begin{pmatrix}
        e^{it} & 0 \\
        0 & e^{iat} \\
      \end{pmatrix}
      \right\vert
      t \in \R
    \right\}
  .\]%
  Show that
  \[%
    \overline{G} = \left\{\left.
      \begin{pmatrix}
        e^{it} & 0 \\
        0 & e^{is} \\
      \end{pmatrix}
      \right\vert
      t, s \in \R
    \right\}
  ,\]%
  where $\overline{G}$ denotes the closure of the set $G$ inside the space of $2
  \times 2$ matrices.

  \textit{Note}: The group $\overline{G}$ can be thought of as the torus $S^1
  \times S^1$, which in turn can be thought of as $[0, 2\pi] \times [0, 2\pi]$,
  with the ends of the intervals identified. The set $G \subset [0, 2\pi] \times
  [0, 2\pi]$ is called an \textbf{irrational line}. Draw a picture of this set
  and you should see why $G$ is dense in $[0, 2\pi] \times [0, 2\pi]$.
\end{exercise}

\begin{proof}[Solution]
  We first show that the set $\left\{e^{2\pi ina}\right\}$ is dense in $S^1$. We
  want to show that
  \[%
    \overline{\left\{e^{2\pi ina}\right\}} = S^1
  .\]%
  Define the sequence $\theta_n = na \pmod{1}$, which lives in the unit interval
  $[0, 1)$. The sequence $\left\{e^{2\pi i\theta_n}\right\}$ consists of points
  on the unit circle. Since $a$ is irrational, the sequence $\theta_n$ is dense
  in $[0, 1]$. Since the mapping $\theta \mapsto e^{2\pi i\theta}$ is a
  continuous function from $[0, 1]$ to $S^1$, it follows that the sequence
  $\left\{e^{2\pi ina}\right\}$ is dense in $S^1$. Therefore, for every
  $\epsilon > 0$, we can find an $n$ such that $e^{2\pi ina}$ is within
  $\epsilon$ of $e^{2\pi i\theta}$.

  Now, we can show that $G$ is dense in $S^1 \times S^1$. The elements of $G$
  are determined by pairs of $(t, at)$, which trace out points in the torus $S^1
  \times S^1$. That is, we're looking at the set
  \[%
    \left\{\left.t \pmod{2\pi}, at \pmod{2\pi} \right\vert t \in \R\right\} \subset [0, 2\pi] \times [0, 2\pi]
  .\]%
  Since $a$ is irrational, the set of points $(t \pmod{2\pi}, at \pmod{2\pi})$
  is dense in $[0, 2\pi] \times [0, 2\pi]$, since the second coordinate, $at
  \pmod{2\pi}$ acts are an irrational rotation of the first coordinate. Since
  the torus $S^1 \times S^1$ is exactly the set of matrices of the form
  \[%
    \left\{
      \begin{pmatrix}
        e^{it} & 0 \\
        0 & e^{is} \\
      \end{pmatrix}
      \bigg\vert
      t, s \in \R
    \right\}
  ,\]%
  and our set is dense in this torus, it follows that the closure of $G$ is
  precisely
  \[%
    \overline{G} = \left\{\left.
      \begin{pmatrix}
        e^{it} & 0 \\
        0 & e^{is} \\
      \end{pmatrix}
      \right\vert
      t, s \in \R
    \right\}
  .\qedhere\]%
\end{proof}

\begin{exercise}[2]
  \textit{Orthogonal Group.} Let $\langle \cdot, \cdot \rangle$ denote the
  standard inner product on $\R^n$, $\langle x, y \rangle = \sum_i x_iy_i$. Show
  that a matrix $A$ preserves inner products if and only if the column vectors
  of $A$ are orthonormal.

  Show that for any $n \times n$ real matrix $B$,
  \[%
    \langle Bx, y \rangle = \langle x, B^Ty \rangle
  ,\]%
  where $\left(B^T\right)_{ij} = B_{ji}$. Using this fact, show that a matrix
  $A$ preserves inner products if and only if $A^TA = I$.

  \textit{Note:} a similar analysis applies to the complex orthogonal groups
  $\Or(n, \C)$ and $\SO(n, \C)$.
\end{exercise}

\begin{proof}[Solution]
  We first show that a matrix $A$ preserves inner products if and only if the
  column vectors of $A$ are orthonormal.

  Assume $A$ preserves inner products. The standard inner product is defined as
  $\langle x, y \rangle = x^Ty$. Since $A$ preserves inner products, that means
  $\langle Ax, Ay \rangle = (Ax)^TAy = x^TA^TAy = x^Ty$. This means $A^T =
  A^{-1}$, since we require $A^TA = I$ to preserve inner products. Let $A =
  \left(\v_1~\v_2~\cdots~\v_n\right)$, where $\v_i$ are the column vectors of
  $A$. Then, we have
  \begin{equation}\label{eq:orthonormal}
    A^TA = \begin{pmatrix}
      \v_1^T \\
      \v_2^T \\
      \cdots \\
      \v_n^T \\
    \end{pmatrix}
    \begin{pmatrix}
      \v_1 & \v_2 & \cdots & \v_n \\
    \end{pmatrix}
    = \begin{pmatrix}
      \langle \v_1, \v_1 \rangle & \langle \v_1, \v_2 \rangle & \cdots & \langle \v_1, \v_n \rangle \\
      \langle \v_2, \v_1 \rangle & \langle \v_2, \v_2 \rangle & \cdots & \langle \v_2, \v_n \rangle \\
      \vdots & \vdots & \ddots & \vdots \\
      \langle \v_n, \v_1 \rangle & \langle \v_n, \v_2 \rangle & \cdots & \langle \v_n, \v_n \rangle \\
    \end{pmatrix} = I
  \end{equation}
  Therefore, we must have $\langle \v_i, \v_j \rangle = \delta_{ij}$, for $i, j
  = 1, \cdots, n$. Therefore, the column vectors of $A$ are orthonormal.

  Now, assume the columns of $A$ are orthonormal. Same thing as shown in
  equation \ref{eq:orthonormal}. Therefore, we have
  \[%
    \langle Ax, Ay \rangle = (Ax)^TBy = x^TA^TAy = x^Ty
  .\]%
  Therefore, $A$ preserves inner products.

  Thus, we have shown that a matrix $A$ preserves inner products if and only if
  the column vectors of $A$ are orthonormal.

  Now, we show that $\langle Bx, y \rangle = \langle x, B^Ty \rangle$. Notice
  that
  \[%
    \langle Bx, y \rangle = (Bx)^Ty = x^TB^Ty = \langle x, B^Ty \rangle
  .\]%

  Since we have proven that $A$ preserves inner products if and only if $A^TA =
  I$
\end{proof}

\begin{exercise}[3]
  \textit{Unitary groups.} Let $\langle \cdot, \cdot \rangle$ denote the
  standard inner product on $\C^n$, $\langle x, y \rangle = \sum_i
  \overline{x_i}y_i$. Following Exercise 2, show that $A^*A = I$ if and only if
  $\langle Ax, Ay \rangle = \langle x, y \rangle$ for all $x, y \in \C^n$.
  ($(A^*)_{ij} = \overline{A_{ji}}$).
\end{exercise}

\begin{proof}[Solution]
  We first show that $A^*A = I$ if and only if $\langle Ax, Ay \rangle =
  \langle x, y \rangle$ for all $x, y \in \C^n$.

  Assume $A^*A = I$. The standard inner product is defined as $\langle x, y
  \rangle = \sum_i \overline{x_i}y_i$. Since $A$ preserves inner products, that
  means $\langle Ax, Ay \rangle = (Ax)^*Ay = x^*A^*Ay = x^*y = \langle x, y
  \rangle$. Therefore, we have shown that $A^*A = I$ implies $\langle Ax, Ay
  \rangle = \langle x, y \rangle$.

  Now, assume $\langle Ax, Ay \rangle = \langle x, y \rangle$. We have
  \[%
    \langle Ax, Ay \rangle = (Ax)^*Ay = x^*A^*Ay = x^*y = \langle x, y \rangle
  .\]%
  Therefore, $A^*A = I$.

  Thus, we have shown that $A^*A = I$ if and only if $\langle Ax, Ay \rangle =
  \langle x, y \rangle$ for all $x, y \in \C^n$.
\end{proof}

\begin{exercise}[4]
  \textit{Generalized orthogonal groups.} Let $[x, y]_{n,k}$ be the symmetric
  bilinear form on $\R^{n+k}$ defined in 2.1. Let $gj$ be the $(n + k) \times (n
  + k)$ diagonal matrix with first $n$ diagonal entries equal to one, and last
  $k$ diagonal entries equal to minus one
  \[%
    g = \begin{pmatrix}
      I_n & 0 \\
      0 & -I_k \\
    \end{pmatrix}
  .\]%
  Show that for all $x, y \in \R^{n+k}$,
  \[%
    [x, y]_{n,k} = \langle x, gy \rangle
  .\]%
  Show that a $(n + k) \times (n + k)$ real matrix $A$ is in $\Or(n, k)$ if and
  only if $A^TgA = g$. Show that $\Or(n, k)$ and $\SO(n, k)$ are subgroups of
  $\GL(n + k, \R)$, and are matrix Lie groups.
\end{exercise}

\begin{proof}[Solution]
  We first show that $[x, y]_{n,k} = \langle x, gy \rangle$. By definition, the symmetric bilinear form $[x, y]_{n,k}$ is given by
  \[
    [x, y]_{n,k} = x^T g y.
  \]
  We also have the standard Euclidean inner product
  \[%
    \langle x, y \rangle = x^Ty
  .\]%
  Since $g$ is the diagonal matrix
  \[%
    g = \begin{pmatrix}
      I_n & 0 \\
      0 & -I_k \\
    \end{pmatrix}
  ,\]%
  it follows that $\langle x, gy \rangle = x^T(gy)$. By substituting $g$, we get $\langle x, gy \rangle = x^Tgy$. Since this matches the definition of $[x, y]_{n,k}$, we conclude that
  \[%
    [x, y]_{n,k} = \langle x, gy \rangle
  .\]%

  Now, we show that a $(n + k) \times (n + k)$ real matrix $A$ is in $\Or(n, k)$ if and only if $A^TgA = g$.

  Assume $A \in \Or(n, k)$. By definition, $A$ preserves the bilinear form,
  meaning that for all $x, y \in \R^{n+k}$, we have
  \[%
    [Ax, Ay]_{n,k} = [x, y]_{n,k}
  .\]%
  Expanding this using the definition of the bilinear form,
  \[%
    (Ax)^Tg(Ay) = x^Tgy
  .\]%
  Since this holds for all $x, y$, it follows that
  \[%
    A^TgA = g
  .\]%
  Conversely, if $A^T g A = g$, then for any $x, y \in \R^{n+k}$,
  \[%
    (Ax)^Tg(Ay) = x^TA^TgAy = x^Tgy = [x, y]_{n,k}
  ,\]%
  which implies that $A$ preserves the bilinear form.

  Thus, we have shown that a $(n + k) \times (n + k)$ real matrix $A$ is in
  $\Or(n, k)$ if and only if $A^TgA = g$.

  Now, we show that $\Or(n, k)$ and $\SO(n, k)$ are subgroups of $\GL(n + k,
  \R)$ and are matrix Lie groups. We check closure, identity, and inverses.

  If $A, B \in \Or(n, k)$, then
  \[%
    (AB)^Tg(AB) = B^T(A^TgA)B = B^TgB = g
  .\]%
  Hence, $AB \in \Or(n, k)$, so $\Or(n, k)$ is closed under multiplication.

  The identity matrix $I_{n+k}$ satisfies $I^TgI = g$, so $I \in \Or(n, k)$.

  If $A \in \Or(n, k)$, then $A^T g A = g$. Multiplying on the right by $A^{-1}$
  and on the left by $A^{-T}$, we obtain $A^{-T} g = g A^{-1}$ Taking transposes
  gives $(A^{-1})^T g A^{-1} = g$, so $A^{-1} \in \Or(n, k)$.

  Thus, $\Or(n, k)$ is a subgroup of $\GL(n+k, \R)$.

  For the special indefinite orthogonal group, $\SO(n, k)$, we impose the
  additional determinant condition
  \[%
    \SO(n, k) = \left\{A \in \Or(n, k) \mid \det(A) = 1\right\}
  .\]%
  Since the determinant of a product satisfies $\det(AB) = \det(A) \cdot
  \det(B)$, the determinant condition is preserved under multiplication and
  inverses, ensuring that $\SO(n, k)$ is also a subgroup.

  A matrix Lie group is a subgroup of $\GL(n+k, \R)$ that is also a smooth
  submanifold of $\R^{(n+k)^2}$. The defining equation $A^TgA = g$ imposes
  $\sfrac{(n+k)(n+k+1)}{2}$ independent constraints on the $(n + k)^2$ entries
  of $A$, since $A^TgA - g = 0$ is a symmetric matrix equation. The set of
  solutions forms a submanifold of $\R^{(n+k)^2}$. The determinant condition
  $\det(A) = 1$ is a smooth function, defining $\SO(n, k)$ as a submanifold of
  $\Or(n, k)$.

  Since both are smooth manifolds and closed under multiplication and inversion,
  they are matrix Lie groups.
\end{proof}

\begin{exercise}[5]
  \textit{Symplectic groups.} Let $B[x, y]$ be the skew-symmetric bilinear form
  on $\R^{2^n}$ given by $B[x, y] = \sum_{i=1}^n x_iy_{n+1} - x_{n+i}y_i$. Let
  $J$ be the $2n \times 2n$ matrix
  \[%
    J = \begin{pmatrix}
      0 & I_n \\
      -I_n & 0 \\
    \end{pmatrix}
  .\]%
  Show that for all $x, y \in \R^{2n}$,
  \[%
    B[x, y] = \langle x, Jy \rangle
  .\]%
  Show that a $2n \times 2n$ matrix $A$ is in $\Sp(n, \R)$ if and only if $A^TJA
  = J$. Show that $\Sp(n, \R)$ is a subgroup of $\GL(2n, \R)$, and a matrix Lie
  group.

  \textit{Note:} a similar analysis applies to $\Sp(n, \C)$.
\end{exercise}

\begin{proof}[Solution]
  We first show that $B[x, y] = \langle x, Jy \rangle$. By definition, the
  skew-symmetric bilinear form $B[x, y]$ is given by
  \[%
    B[x, y] = \sum_{i=1}^n x_i y_{n+i} - x_{n+i} y_i
  .\]%
  The standard Euclidean inner product is given by $\langle x, y \rangle = x^T
  y$ Since $J$ is defined as
  \[%
    J = \begin{pmatrix}
      0 & I_n \\
      -I_n & 0 \\
    \end{pmatrix}
  ,\]%
  we compute $\langle x, Jy \rangle = x^T (Jy)$. Expanding using the structure
  of $J$,
  \[%
    Jy = \begin{pmatrix}
      0 & I_n \\
      -I_n & 0 \\
    \end{pmatrix}
    \begin{pmatrix}
      y_1 \\
      y_2 \\
    \end{pmatrix}
    = \begin{pmatrix}
      y_2 \\
      -y_1 \\
    \end{pmatrix}
  ,\]%
  so that
  \[%
    x^T Jy = (x_1^T x_2^T) \begin{pmatrix}
      y_2 \\
      -y_1 \\
    \end{pmatrix} = x_1^T y_2 - x_2^T y_1 = \sum_{i=1}^n x_i y_{n+i} - x_{n+i} y_i
  .\]%
  This matches the definition of $B[x, y]$, proving the first statement.

  Now, we show that a $2n \times 2n$ matrix $A$ is in $\Sp(n, \R)$ if and only
  if $A^T J A = J$.

  Assume $A \in \Sp(n, \R)$. By definition, $A$ preserves the bilinear form,
  meaning that for all $x, y \in \R^{2n}$, $B[Ax, Ay] = B[x, y]$. Expanding this
  using the definition of the bilinear form, $(Ax)^T J (Ay) = x^TJy$. Since this
  holds for all $x, y$, it follows that $A^TJA = J$. Conversely, if $A^T J A =
  J$, then for any $x, y \in \R^{2n}$,
  \[%
    (Ax)^TJ(Ay) = x^TA^TJAy = x^TJy = B[x, y]
  ,\]%
  which implies that $A$ preserves the bilinear form. Thus, we have shown that
  $A \in \Sp(n, \R)$ if and only if $A^T J A = J$.

  Finally, we show that $\Sp(n, \R)$ is a subgroup of $\GL(2n, \R)$ and a matrix
  Lie group. Again, we check closure, identity, and inverses.

  If $A, B \in \Sp(n, \R)$, then
  \[%
    (AB)^T J (AB) = B^T (A^T J A) B = B^T J B = J
  ,\]%
  so $AB \in \Sp(n, \R)$.

  The identity matrix $I_{2n}$ satisfies $I^T J I = J$, so $I \in \Sp(n, \R)$.

  If $A \in \Sp(n, \R)$, then $A^TJA = J$. Multiplying on the right by $A^{-1}$
  and on the left by $A^{-T}$, we obtain $A^{-T}J = JA^{-1}$. Taking transposes
  gives $(A^{-1})^T J A^{-1} = J$, so $A^{-1} \in \Sp(n, \R)$.

  Thus, $\Sp(n, \R)$ is a subgroup of $\GL(2n, \R)$.

  To show that $\Sp(n, \R)$ is a matrix Lie group, we note that the defining
  equation $A^TJA = J$ imposes $n(2n + 1)$ independent constraints on the $4n^2$
  entries of $A$, forming a submanifold of $\R^{4n^2}$. Since it is also closed
  under multiplication and inversion, $\Sp(n, \R)$ is a matrix Lie group.
\end{proof}

\begin{exercise}[6]
  \textit{The groups $\Or(2)$ and $\SO(2)$.} Show that the matrix
  \[%
    A = \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
  ,\]%
  is in $\SO(2)$, and that
  \[%
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
    \begin{pmatrix}
      \cos(\phi) & -\sin(\phi) \\
      \sin(\phi) & \cos(\phi) \\
    \end{pmatrix}
    = \begin{pmatrix}
      \cos(\theta + \phi) & -\sin(\theta + \phi) \\
      \sin(\theta + \phi) & \cos(\theta + \phi) \\
    \end{pmatrix}
  .\]%
  Show that every element $A$ of $\Or(2)$ is of one of the two forms
  \begin{align*}
    A &= \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix} \\
    A &= \begin{pmatrix}
      \cos(\theta) & \sin(\theta) \\
      \sin(\theta) & -\cos(\theta) \\
    \end{pmatrix}
  .\end{align*}
  (If $A$ is of the first form, then $\det(A) = 1$; if $A$ is of the second
  form, then $\det(A) = -1$.)

  \textit{Hint:} Recall that for $A = \begin{pmatrix}
    a & b \\
    c & d \\
  \end{pmatrix}$ to be in $\Or(2)$, the column vectors $\begin{pmatrix}
    a \\
    c \\
  \end{pmatrix}$ and $\begin{pmatrix}
    b \\
    d \\
  \end{pmatrix}$ must be unit vectors, and must be orthogonal.
\end{exercise}

\begin{proof}[Solution]
  We first show that the given matrix is in $\SO(2)$. A matrix is in $\SO(2)$ if
  it is orthogonal and has determinant 1. Consider
  \[%
    A = \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
  .\]%
  The determinant is computed as
  \[%
    \det(A) = \cos^2(\theta) + \sin^2(\theta) = 1
  .\]%
  The transpose of $A$ is
  \[%
    A^T = \begin{pmatrix}
      \cos(\theta) & \sin(\theta) \\
      -\sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
  .\]%
  We verify that $A^TA = I$
  \[%
    A^T A = \begin{pmatrix}
      \cos(\theta) & \sin(\theta) \\
      -\sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix} = I
  .\]%
  Hence, $A \in \SO(2)$.

  Next, we prove the composition rule,
  \[%
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
    \begin{pmatrix}
      \cos(\phi) & -\sin(\phi) \\
      \sin(\phi) & \cos(\phi) \\
    \end{pmatrix}
  .\]%
  Performing matrix multiplication,
  \[%
    \begin{pmatrix}
      \cos(\theta)\cos(\phi) - \sin(\theta)\sin(\phi) & -\cos(\theta)\sin(\phi) - \sin(\theta)\cos(\phi) \\
      \sin(\theta)\cos(\phi) + \cos(\theta)\sin(\phi) & -\sin(\theta)\sin(\phi) + \cos(\theta)\cos(\phi) \\
    \end{pmatrix}
  .\]%
  Using angle sum identities,
  \[%
    \begin{pmatrix}
      \cos(\theta + \phi) & -\sin(\theta + \phi) \\
      \sin(\theta + \phi) & \cos(\theta + \phi) \\
    \end{pmatrix}
  .\]%
  Thus, matrix multiplication corresponds to addition of angles.

  Now, we classify all elements of $\Or(2)$. Suppose $A \in \Or(2)$, so its
  column vectors must be orthonormal. Let
  \[%
    A = \begin{pmatrix}
      a & b \\
      c & d \\
    \end{pmatrix}
  .\]%
  The orthogonality conditions are
  \begin{align*}
    a^2 + c^2 &= 1 \\
    b^2 + d^2 &= 1 \\
    a b + c d &= 0
  .\end{align*}
  Setting $a = \cos(\theta)$ and $c = \sin(\theta)$, we solve for $b$ and $d$.
  The two possible solutions are,
  \[%
    A = \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta) \\
    \end{pmatrix}
  .\]%
  which has determinant 1, and
  \[%
    A = \begin{pmatrix}
      \cos(\theta) & \sin(\theta) \\
      \sin(\theta) & -\cos(\theta) \\
    \end{pmatrix}
  ,\]%
  which has determinant $-1$. These two cases classify all elements of $\Or(2)$.
\end{proof}

\begin{exercise}[7]
  \textit{The groups $\Or(1, 1)$ and $\SO(1, 1)$.} Show that
  \[%
    A = \begin{pmatrix}
      \cosh(t) & \sinh(t ) \\
      \sinh(t) & \cosh(t ) \\
    \end{pmatrix}
  ,\]%
  is in $\SO(1, 1)$, and that
  \[%
    \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
    \begin{pmatrix}
      \cosh(s) & \sinh(s) \\
      \sinh(s) & \cosh(s) \\
    \end{pmatrix}
    = \begin{pmatrix}
      \cosh(t + s) & \sinh(t + s) \\
      \sinh(t + s) & \cosh(t + s) \\
    \end{pmatrix}
  .\]%
  Show that every element of $\Or(1, 1)$ can be written in one of the four forms
  \begin{align*}
    A &= \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix} \\
    A &= \begin{pmatrix}
      -\cosh(t) & \sinh(t) \\
      \sinh(t) & -\cosh(t) \\
    \end{pmatrix} \\
    A &= \begin{pmatrix}
      \cosh(t) & -\sinh(t) \\
      \sinh(t) & -\cosh(t) \\
    \end{pmatrix} \\
    A &= \begin{pmatrix}
      -\cosh(t) & -\sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
  .\end{align*}
  (Since $\cosh(t)$ is always positive, there is no overlap among the four
  cases. Matrices of the first two forms have determinant one, matrices of the
  last two forms have determinant minus one.)

  \textit{Hint:} For $
  \begin{pmatrix}
    a & b \\
    c & d \\
  \end{pmatrix}$ to be in $\Or(1, 1)$, we must have $a^2 - c^2 = 1$, $b^2 - d^2
  = -1$, and $ad - cd = 0$. The set of points $(a, c)$ in the plane with $a^2 -
  c^2 = 1$ (i.e., $a = \pm\sqrt{1 + c^2}$) is a hyperbola.
\end{exercise}

\begin{proof}[Solution]
  We first show that the given matrix is in $\SO(1,1)$. A matrix is in
  $\SO(1,1)$ if it preserves the quadratic form $x^2 - y^2$ and has determinant
  1. Consider
  \[%
    A = \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
  .\]%
  The determinant is computed as
  \[%
    \det(A) = \cosh^2(t) - \sinh^2(t) = 1
  .\]%
  We verify that $A$ preserves the quadratic form, $A^TJA = J$,
  where $J = \begin{pmatrix}
    1 & 0 \\
    0 & -1 \\
  \end{pmatrix}$. Computing,
  \[%
    A^T J A = \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}^T
    \begin{pmatrix}
      1 & 0 \\
      0 & -1 \\
    \end{pmatrix}
    \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
  .\]%
  Simplifying, we find that $A$ satisfies the defining property of $\SO(1,1)$.

  Next, we prove the composition rule,
  \[%
    \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
    \begin{pmatrix}
      \cosh(s) & \sinh(s) \\
      \sinh(s) & \cosh(s) \\
    \end{pmatrix}
  .\]%
  Performing matrix multiplication,
  \[%
    \begin{pmatrix}
      \cosh(t)\cosh(s) + \sinh(t)\sinh(s) & \cosh(t)\sinh(s) + \sinh(t)\cosh(s) \\
      \sinh(t)\cosh(s) + \cosh(t)\sinh(s) & \sinh(t)\sinh(s) + \cosh(t)\cosh(s) \\
    \end{pmatrix}
  .\]%
  Using hyperbolic identities,
  \[%
    \begin{pmatrix}
      \cosh(t+s) & \sinh(t+s) \\
      \sinh(t+s) & \cosh(t+s) \\
    \end{pmatrix}
  .\]%
  Thus, matrix multiplication corresponds to addition of parameters.

  Now, we classify all elements of $\Or(1,1)$. Suppose $A \in \Or(1,1)$, so it
  must satisfy the conditions:
  \begin{align*}
    a^2 - c^2 &= 1 \\
    b^2 - d^2 &= -1 \\
    ad - bc &= 0
  .\end{align*}
  Solving for valid forms, we obtain four cases
  \[%
    A = \begin{pmatrix}
      \cosh(t) & \sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
  \]%
  \[%
    A = \begin{pmatrix}
      -\cosh(t) & \sinh(t) \\
      \sinh(t) & -\cosh(t) \\
    \end{pmatrix}
  \]%
  \[%
    A = \begin{pmatrix}
      \cosh(t) & -\sinh(t) \\
      \sinh(t) & -\cosh(t) \\
    \end{pmatrix}
  \]%
  \[%
    A = \begin{pmatrix}
      -\cosh(t) & -\sinh(t) \\
      \sinh(t) & \cosh(t) \\
    \end{pmatrix}
  .\]%
  Since $\cosh(t)$ is always positive, there is no overlap among the four cases.
  Matrices of the first two forms have determinant $1$, while those of the last
  two forms have determinant $-1$. These four cases classify all elements of
  $\Or(1,1)$.
\end{proof}

\begin{exercise}[8]
  \textit{The group $\SU(2)$.} Show that if $\alpha$, $\beta$ are arbitrary
  complex numbers satisfying $\lvert \alpha \rvert^2 + \lvert \beta \rvert^2 =
  1$, then the matrix
  \begin{equation}\label{eq:group_su_form}
    A = \begin{pmatrix}
      \alpha & -\overline{\beta} \\
      \beta & \overline{\alpha} \\
    \end{pmatrix}
  ,\end{equation}
  is in $\SU(2)$. Show that every $A \in \SU(2)$ can be expressed in the form in
  equation \ref{eq:group_su_form} for a unique pair $(\alpha, \beta)$ satisfying
  $\lvert \alpha \rvert^2 + \lvert \beta \rvert^2 = 1$. (Thus $\SU(2)$ can be
  thought of as the three-dimensional sphere $S^3$ sitting inside $\C^2 = \R^4$.
  In particular, this shows that $\SU(2)$ is connected and simply connected.)
\end{exercise}

\begin{proof}[Solution]
  We first show that the given matrix is in $\SU(2)$. A matrix is in $\SU(2)$ if
  it is unitary and has determinant $1$. Consider
  \[%
    A = \begin{pmatrix}
      \alpha & -\overline{\beta} \\
      \beta & \overline{\alpha} \\
    \end{pmatrix}
  .\]%
  The determinant of $A$ is computed as
  \[%
    \det(A) = \alpha \overline{\alpha} - (-\overline{\beta}) \beta = \lvert \alpha \rvert^2 + \lvert \beta \rvert^2 = 1
  .\]%
  The conjugate transpose of $A$ is
  \[%
    A^* = \begin{pmatrix}
      \overline{\alpha} & \overline{\beta} \\
      -\beta & \alpha \\
    \end{pmatrix}
  .\]%
  We verify that $A^* A = I$
  \[%
    A^* A = \begin{pmatrix}
      \overline{\alpha} & \overline{\beta} \\
      -\beta & \alpha \\
    \end{pmatrix}
    \begin{pmatrix}
      \alpha & -\overline{\beta} \\
      \beta & \overline{\alpha} \\
    \end{pmatrix}
    = \begin{pmatrix}
      \overline{\alpha} \alpha + \overline{\beta} \beta & -\overline{\alpha} \overline{\beta} + \overline{\beta} \overline{\alpha} \\
      -\beta \alpha + \alpha \beta & -\beta (-\overline{\beta}) + \alpha \overline{\alpha} \\
    \end{pmatrix}
  .\]%
  Since $\lvert \alpha \rvert^2 + \lvert \beta \rvert^2 = 1$, we obtain $A^* A =
  I$, proving that $A \in \SU(2)$.

  Next, we show that every element of $\SU(2)$ can be written in this form.
  Suppose
  \[%
    B = \begin{pmatrix}
      a & b \\
      c & d \\
    \end{pmatrix} \in \SU(2)
  .\]%
  Since $B$ is unitary, $B^* B = I$ gives the conditions
  \begin{align*}
    \lvert a \rvert^2 + \lvert c \rvert^2 &= 1 \\
    \lvert b \rvert^2 + \lvert d \rvert^2 &= 1 \\
    a \overline{b} + c \overline{d} &= 0
  .\end{align*}
  Since $\det(B) = 1$, we also have $ad - bc = 1$ Setting $a = \alpha$, $c =
  \beta$, $b = -\overline{\beta}$, and $d = \overline{\alpha}$ satisfies all
  these conditions, with the constraint $\lvert \alpha \rvert^2 + \lvert \beta
  \rvert^2 = 1$ ensuring that $B$ is unitary. Uniqueness follows from the linear
  independence of $\alpha$ and $\beta$ as complex numbers.

  This shows that every $A \in \SU(2)$ can be expressed in the given form,
  proving that $\SU(2)$ is topologically equivalent to the $3$-sphere $S^3$ in
  $\C^2 = \R^4$.
\end{proof}

\begin{exercise}[9]
  \textit{The groups $\Sp(1, \R)$, $\Sp(1, \C)$, and $\Sp(1)$.} Show that
  $\Sp(1, \R) = \SL(2, \R)$, $\Sp(1, \C) = \SL(2, \C)$, and $\Sp(1) = \SU(2)$.
\end{exercise}

\begin{proof}[Solution]
  We first show that $\Sp(1, \R) = \SL(2, \R)$. By definition, $\Sp(1, \R)$
  consists of $2 \times 2$ real matrices preserving the symplectic form given by
  \[%
    J = \begin{pmatrix}
      0 & 1 \\
      -1 & 0 \\
    \end{pmatrix}
  .\]%
  A matrix $A \in \Sp(1, \R)$ satisfies $A^T J A = J$. Explicit computation
  shows that this condition is equivalent to requiring that $\det(A) = 1$, which
  is precisely the definition of $\SL(2, \R)$. Thus, we conclude that $\Sp(1,
  \R) = \SL(2, \R)$.

  Next, we show that $\Sp(1, \R) = \SL(2, \R)$. The complex symplectic group
  consists of $2 \times 2$ complex matrices preserving the complex symplectic
  form $J$. Again, the defining relation $A^T J A = J$ reduces to the
  determinant condition $\det(A) = 1$, which characterizes $\SL(2, \R)$. Hence,
  $\Sp(1, \R) = \SL(2, \R)$.

  Finally, we show that $\Sp(1) = \SU(2)$. The quaternionic symplectic group
  $\Sp(1)$ consists of $2 \times 2$ complex matrices preserving both the
  symplectic form $J$ and the Hermitian form given by $A^* A = I$. Since
  $\SU(2)$ consists of $2 \times 2$ unitary matrices with determinant $1$, it
  follows that the preservation conditions defining $\Sp(1)$ are equivalent to
  those of $\SU(2)$. Thus, we conclude that $\Sp(1) = \SU(2)$.
\end{proof}

\begin{exercise}[10]
  \textit{The Heisenberg group.} Determine the center $Z(h)$ of the Heisenberg
  group $H$. Show that the quotient group $H / Z(H)$ is abelian.
\end{exercise}

\begin{proof}[Solution]
  We first determine the center $Z(H)$ of the Heisenberg group $H$. The group
  operation is matrix multiplication. Direct computation shows that the product
  of two elements is given by
  \[%
    \begin{pmatrix}
      1 & a & c \\
      0 & 1 & b \\
      0 & 0 & 1 \\
    \end{pmatrix}
    \begin{pmatrix}
      1 & a' & c' \\
      0 & 1 & b' \\
      0 & 0 & 1 \\
    \end{pmatrix}
    = \begin{pmatrix}
      1 & a + a' & c + c' + ab' \\
      0 & 1 & b + b' \\
      0 & 0 & 1 \\
    \end{pmatrix}
  .\]%
  The center $Z(H)$ consists of all elements that commute with every element of
  $H$. Let
  \[
    X = \begin{pmatrix}
      1 & a & c \\
      0 & 1 & b \\
      0 & 0 & 1 \\
    \end{pmatrix}
    \aand
    Y = \begin{pmatrix}
      1 & a' & c' \\
      0 & 1 & b' \\
      0 & 0 & 1 \\
    \end{pmatrix}.
  \]
  Their commutator is given by
  \[%
    XY Y^{-1} X^{-1} = \begin{pmatrix}
      1 & 0 & ab' - a'b \\
      0 & 1 & 0 \\
      0 & 0 & 1 \\
    \end{pmatrix}
  .\]%
  For this to be the identity matrix, we require $ab' - a'b = 0$ for all $a, b,
  a', b' \in \R$. This implies that $c$ must be the only free parameter in the
  center, meaning that $Z(H) = \left\{\left.\begin{pmatrix}
    1 & 0 & c \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
  \end{pmatrix} \right\rvert c \in \R \right\}$.

  Now, we show that the quotient group $H / Z(H)$ is abelian. Consider the
  natural projection map
  \[%
    \pi: H \to H / Z(H)
  ,\]%
  given by
  \[%
    \pi \left(\begin{pmatrix}
        1 & a & c \\
        0 & 1 & b \\
        0 & 0 & 1 \\
    \end{pmatrix}\right) = (a, b) \in \R^2
  .\]%
  The group operation in $H / Z(H)$ is then given by
  \[%
    (a, b) \cdot (a', b') = (a + a', b + b')
  .\]%
  Since addition in $\R^2$ is commutative, it follows that $H / Z(H)$ is an
  abelian group.
\end{proof}

\begin{exercise}[11]
  \textit{Connectedness of} $\SO(n)$. Show that $\SO(n)$ is connected, following
  the outline below.

  For the $n = 1$ case, there is not much to show, since a $1 \times 1$ matrix
  with determinant one must be 1. Assume, then, that $n \ge 2$ Let $\e_1$ denote
  the vector
  \[%
    \begin{pmatrix}
      1 \\
      0 \\
      \vdots \\
      0
    \end{pmatrix} \in \R^n
  .\]%
  Given any unit vector $\v \in \R^n$, show that there exists a continuous path
  $R(t)$ in $\SO(n)$ such that $R(0) = I$ and $R(1)\e_1 = \v$. (Thus any unit
  vector can be ``continuously rotated'' to $\e_1$.)

  Now show that any element $R$ of $\SO(n)$ can be connected to an element of
  $\SO(n - 1)$, and proceed by induction.
\end{exercise}

\begin{proof}[Solution]
  Let $\B = \{\v, \e_1\}$ be a basis for a two-dimensional plane. By the
  Gram-Schmitt process, we can construct an orthonormal basis $\B' = \{\u_1,
  \u_2\}$ for the same plane. Let $\u_1 = \v$ and $\u_2$ be defined as
  \[%
    \u_2 = \frac{\e_1 - \langle \e_1, \v \rangle \v}{\|\e_1 - \langle \e_1, \v \rangle \v\|}
  .\]%
  Let $\theta(t) : [0, 1] \to \R$, defined by
  \[%
    \theta(0) = 0 \aand \theta(1) = \arccos\left(\frac{\v \cdot \e_1}{\lVert \v \rVert \lVert \e_1 \rVert}\right)
  .\]%
  Then, we can construct a rotation $R(t) \in \SO(n)$ as a block matrix that
  acts as a rotation by $\theta(t)$ in the plane spanned by $\{\u_1, \u_2\}$,
  and as the identity on the orthogonal complement. This defines a continuous
  path with $R(0) = I$ and $R(1) \v = \e_1$.

  We'll use induction to show that $\SO(n)$ is connected for all $n \ge 1$. The
  base case is trivial, which is trivially connected.

  Assume $\SO(n)$ is connected for some $n \geq 1$. We need to show that
  $\SO(n+1)$ is connected. Let $R \in \SO(n+1)$. Consider the first column of
  $R$, which is a unit vector $\v \in \R^{n+1}$.

  Since $R(1)$ maps $t$ to $\e_1$, we consider the path $t \mapsto R(t) T$.
  Observe that
  \[%
     R(1)T \e_1 = R(1) \v = \e_1
  .\]%
  This means that $R(1)T$ is an element of $\SO(n+1)$ whose first column is
  $\e_1$.

  We can write $R(1)T$ as a block matrix,
  \[%
    R(1)T = \begin{pmatrix}
      1 & 0 \\
      0 & T'
    \end{pmatrix}
  ,\]%
  where $T'$ is an element of $\SO(n)$. By the induction hypothesis, there
  exists a continuous path from the identity matrix $I_n$ to $T'$ in $\SO(n)$.
  We now concatenate the paths:
  \begin{enumerate}
    \item The path $R(t)$ from $I_{n+1}$ to $R(1)$,

    \item The path $t \mapsto R(1) T$ from $R(1)$ to $R(1) T$,

    \item The path from $I_n$ to $T'$ in $\SO(n)$, which lifts to a path from
      $I_{n+1}$ to $R(1)T$ in $\SO(n + 1)$.
  \end{enumerate}
  Concatenating these paths gives a continuous path from $I_{n+1}$ to any
  element of $\SO(n + 1)$, proving that $\SO(n + 1)$ is connected.

  By induction, $\SO(n)$ is connected for all $n \geq 1$.
\end{proof}

\begin{exercise}[12]
  \textit{The polar decomposition of $\SL(n, \R)$.} Show that every element $A$
  of $\SL(n, \R)$ can be written uniquely in the form $A = RH$, where $R \in
  \SO(n)$, and $H$ is a symmetric, positive-definite matrix with determinant one
  (That is, $H^T = H$, and $\langle \x, H\x \rangle \ge 0$ for all $\x \in
  \R^n$).

  \textit{Hint:} If $A$ could be written in this form, then we would have
  \[%
    A^TA = H^TR^TRH = HR^{-1}RH = H^2
  .\]%
  Thus $H$ would have to be the unique positive-definite symmetric square root
  of $A^TA$.

  \textit{Note:} A similar argument gives polar decompositions for $\GL(n, \R)$,
  $\SL(n, \C)$, and $\GL(n, \C)$. For example, every element $A$ of $\SL(n, \C)$
  can be written uniquely as $A = UH$, with $U \in \SU(n)$, and $H$ is a
  self-adjoint positive definite matrix with determinant one.
\end{exercise}

\begin{proof}[Solution]
  Consider the matrix $A^T A$, which is symmetric and positive definite because
  for any nonzero vector $\x \in \R^n$, we have
  \[%
    (A^T A)^T = (A)^T(A^T)^T = A^T A \aand \x^T A^T A\x = (A\x)^T (A\x) = \lVert A\x \rVert^2 > 0
  .\]%
  Since $A \in \SL(n, \R)$, we have $\det(A) = 1$, implying $\det(A^T A) =
  \det(A^T) \cdot \det(A) = 1 \cdot 1 = 1$ since determinant is multiplicative.
  By the spectral theorem, $A^T A$ has an orthonormal eigenbasis with positive
  eigenvalues, so it admits a unique positive-definite square root, denoted $H$,
  such that
  \[%
    H = \sqrt{A^T A} \implies H^2 = A^T A
  .\]%
  Define $R = AH^{-1}$. We check that $R$ is orthogonal
  \[%
    R^T R = (H^{-1} A^T) (A H^{-1}) = H^{-1} A^T A H^{-1} = H^{-1} H^2 H^{-1} = I
  .\]%
  Thus, $R \in \SO(n)$ since $\det(R) = \sfrac{\det(A)}{\det(H)} = \sfrac{1}{1}
  = 1$, proving existence.

  Suppose $A = R_1 H_1 = R_2 H_2$ are two such decompositions. Then,
  \[%
    H_1^{-1} R_1^{-1} R_2 H_2 = I
  .\]%
  Multiplying on the right by $H_2^{-1}$ and on the left by $H_1$, we obtain
  \[%
    H_1 H_1^{-1} R_1^{-1} R_2 H_2 H_2^{-1} = H_1 H_2^{-1} = I
  ,\]%
  so $H_1 = H_2$. This implies $R_1 = R_2$, proving uniqueness.

  Thus, every element of $\SL(n, \R)$ has a unique polar decomposition.
\end{proof}

\begin{exercise}[13]
  \textit{The connectedness of $\SL(n, \R)$.} Using the polar decomposition of
  $\SL(n, \R)$ and the connectedness of $\SO(n)$, show that $\SL(n, \R)$ is
  connected.

  \textit{Hint:} Recall that if $H$ is a real, symmetric matrix, then there
  exists a \textit{real} orthogonal matrix $R_1$ such that $H = R_1DR_1^{-1}$,
  where $D$ is diagonal.
\end{exercise}

\begin{proof}[Solution]
  Since we are dealing with $\SL(n, \R)$, we add the restriction that $H$ is of
  determinant one. By the polar decomposition, we can write $A = RH$, where $R
  \in \SO(n)$ and $H$ is a symmetric, positive-definite matrix with determinant
  one.

  Also, by the hint, we can write $H = R_1 D R_1^{-1}$, where $R_1 \in \Or(n)$
  and $D$ is a diagonal matrix. The space of symmetric matrices with determinant
  1 that are also positive definite forms a connected space. This follows
  because the space of positive-definite diagonal matrices with determinant 1 is
  connected, and conjugation by an orthogonal matrix does not change
  connectivity.

  By exercise 11, we know that $\SO(n)$ is connected. Since each element in
  $\SL(n, \R)$ can be written as $RH$, where $R \in \SO(n)$ and $H$ belongs to a
  connected space, and the product of connected spaces is connected, we conclude
  that $\SL(n, \R)$ is connected.
\end{proof}

\begin{exercise}[14]
  \textit{The connectedness of $\GL(n, \R)^+$.} Show that $\GL(n, \R)^+$ is
  connected.
\end{exercise}

\begin{proof}[Solution]
  For any $A \in \GL(n, \R)$, the polar decomposition expresses $A$ uniquely as
  $A = U_A P_A$, $U_A \in \Or(n)$ (i.e., $U_A U_A^T = I$), and $P_A$ is a
  symmetric positive-definite matrix (i.e., $P_A = \sqrt{A^\top A}$, and $P_A$
  has only positive eigenvalues).

  Since $A, B \in \GL(n, \R)^+$, we know that $\det(A) > 0$ and $\det(B) > 0$,
  which implies $U_A, U_B$ have determinant $+1$, so $U_A, U_B \in \SO(n)$.

  Given $A, B \in \GL(n, \R)^+$ with their polar decompositions $A = U_A P_A$
  and $B = U_B P_B$, we can construct a continuous path from $A$ to $B$ as
  follows. Since $\SO(n)$ is path-connected, there exists a smooth path $U_t$ in
  $\SO(n)$ such that $U_0 = U_A$ and $U_1 = U_B$. One explicit choice is the
  geodesic interpolation, $U_t = U_A \exp\left(t \log\left(U_A^\top
  U_B\right)\right)$, which remains in $\SO(n)$ for all $t \in [0,1]$. Since the
  space of symmetric positive-definite matrices is also path-connected, we use
  the interpolation $P_t = (1 - t) P_A + t P_B$. This remains positive definite
  for all $t \in [0,1]$ because the sum of two positive-definite matrices with
  positive weights remains positive definite. Now, we can define the path $A_t =
  U_t P_t$, for $t \in [0,1]$ Since $U_t$ remains in $\SO(n)$ and $P_t$ remains
  positive definite, each $A_t$ is invertible with $\det(A_t) > 0$, ensuring
  $A_t \in \GL(n, \R)^+$ for all $t$.

  Verifying continuity, we get:
  \begin{enumerate}
    \item The function $t \mapsto U_t$ is continuous because it is constructed
      from matrix exponentiation, which is smooth.

    \item The function $t \mapsto P_t$ is trivially continuous as it is a convex
      combination of continuous matrices.

    \item Since matrix multiplication is continuous, the final path $t \mapsto
      A_t = U_t P_t$ is continuous.
  \end{enumerate}

  Thus, $\GL(n, \R)^+$ is connected.
\end{proof}

\begin{exercise}[15]
  Show that the set of translations is a normal subgroup of the Euclidean group,
  and also of the Poincar\'e group. Show that $(\Ec(n)/\textrm{translations})
  \cong \Or(n)$.
\end{exercise}

\begin{proof}[Solution]
  We begin by proving that the set of translations forms a normal subgroup of
  the Euclidean group $\Ec(n)$ and the Poincar\'e group.

  The Euclidean group $\Ec(n)$ consists of all isometries of $\R^n$, which can be
  written as affine transformations
  \[%
    g(x) = Ax + b, \quad A \in \Or(n), \quad b \in \R^n
  .\]%
  The subgroup of translations consists of elements of the form
  \[%
    T_b(x) = x + b, \quad b \in \R^n
  .\]%
  Given any translation $T_b$ and an isometry $g(x) = Ax + c \in \Ec(n)$, we
  compute the conjugation
  \[%
    g T_b g^{-1}(x) = g(T_b(g^{-1}(x)))
  .\]%
  Since $g^{-1}(x) = A^{-1}x - A^{-1}c$, we obtain
  \[%
    T_b(g^{-1}(x)) = A^{-1}x - A^{-1}c + b
  .\]%
  Applying $g$ gives
  \[%
    g T_b g^{-1}(x) = A(A^{-1}x - A^{-1}c + b) + c = x + Ab
  .\]%
  Since $Ab$ is still a translation, we conclude that the set of translations is
  normal in $\Ec(n)$.

  Similarly, in the Poincar\'e group (which consists of Lorentz transformations
  and translations in Minkowski space), the same argument applies, replacing
  $\Or(n)$ with the Lorentz group $O(1,n - 1)$ and showing that translations
  remain normal under conjugation.

  Next, we show that $\Ec(n)/\text{translations} \cong \Or(n)$. Define a map
  \[%
    \varphi: \Ec(n) \to \Or(n), \quad \varphi(A, b) = A
  .\]%
  This is a homomorphism with kernel consisting precisely of the translations
  (i.e., those transformations where $A = I$). Since the first isomorphism
  theorem states that
  \[%
    \Ec(n)/\ker(\varphi) \cong \Im(\varphi)
  .\]%
  we conclude that
  \[%
    \Ec(n)/\text{translations} \cong \Or(n)
  .\qedhere\]%
\end{proof}

\begin{exercise}[16]
  \textit{Harder.} Show that every Lie group homomorphism $\phi : \R \to S^1$ is
  of the form $\phi(x) = e^{iax}$ for some $a \in \R$. In particular, every such
  homomorphism is smooth.
\end{exercise}

\begin{proof}[Solution]
  A Lie group homomorphism $\phi: \R \to S^1$ is a smooth map that preserves the
  group structure, meaning
  \[%
    \phi(x+y) = \phi(x) \phi(y) \quad \forall x, y \in \R
  .\]%
  Since $\R$ is an additive group and $S^1 = \{e^{i\theta} \mid \theta \in \R\}$
  is a multiplicative group, we seek to determine the structure of such a
  homomorphism.

  Define $\theta(x) \in \R$ such that $\phi(x) = e^{i\theta(x)}\phi(x) =
  e^{i\theta(x)}$ The homomorphism property then implies
  \[%
    e^{i\theta(x+y)} = e^{i\theta(x)} e^{i\theta(y)}
  ,\]%
  which simplifies to
  \[%
    \theta(x+y) = \theta(x) + \theta(y) \pmod{2\pi}
  .\]%
  This means $\theta$ is a group homomorphism from $(\R, +)$ to $(\R/2\pi\Z,
  +)$. Since $\R$ is torsion-free, $\theta$ must be of the form $\theta(x) = ax$
  for some $a \in \R$. Thus, the general form of $\phi$ is $\phi(x) = e^{iax}$.

  Since $\phi(x)$ is expressed as a smooth function of $x$, it follows that
  every such Lie group homomorphism is smooth.
\end{proof}
