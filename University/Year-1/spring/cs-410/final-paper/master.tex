\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[toc,page]{appendix}

\usepackage{amsmath}
\let\iint\relax
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{breqn}
\usepackage{upgreek}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{cleveref}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}

\newcommand\sfrac[2]{#1/#2}
\newcommand\aand{\quad\text{and}\quad}

\newcommand\F{\mathcal{F}}
\newcommand\K{\mathcal{K}}
\newcommand\M{\mathcal{M}}
\newcommand\R{\mathbb{R}}
\newcommand\VR{\mathrm{VR}}

\let\div\relax
\DeclareMathOperator\div{div}

\title{
  Manifold Hypothesis and Consistent Manifold Reconstruction in Topological Data
  Analysis
}

\author{
  Hashem A.~Damrah\\
  Department of Mathematics\\
  University of Oregon\\
  Eugene, OR 97403 \\
  \texttt{hdamrah@uoregon.edu} \\
  \And
  Mateo C.~Davis\\
  Department of Mathematics\\
  University of Oregon\\
  Eugene, OR 97403 \\
  \texttt{EMAIL} \\
  \And
  Jakob Rode \\
  Department of CS and Mathematics\\
  University of Oregon\\
  Eugene, OR 97403 \\
  \texttt{EMAIL} \\
}

\renewcommand{\headeright}{
  Manifold Hypothesis and Consistent Manifold Reconstruction
}
\renewcommand{\undertitle}{
  Final Project – CS 410/510: Topological Methods in Data Analysis
}
\renewcommand{\shorttitle}{}

\hypersetup{
  pdftitle={Manifold Hypothesis and Consistent Manifold Reconstruction in Topological Data Analysis},
  pdfsubject={q-bio.NC, q-bio.QM},
  pdfauthor={Hashem A.~Damrah, Mateo C.~Davis, Jakob~Rode},
  pdfkeywords={Manifold, Manifold Hypothesis, Topological Data Analysis, Consistent Manifold Reconstruction, TDA, TDA Consistency},
}

% Symbols
\let\oldlimsymbol\lim\renewcommand\lim{\displaystyle\oldlimsymbol}
\let\to\rightarrow
\let\implies\Rightarrow
\let\impliedby\Leftarrow
\let\iff\Leftrightarrow
\let\epsilon\varepsilon
\let\phi\varphi
\let\tau\uptau

\begin{document}
  \maketitle

  \begin{abstract}
    The manifold hypothesis posits that high-dimensional real-world data often lies near a low-dimensional manifold embedded in ambient space. While this assumption underpins many techniques in machine learning and data analysis, its practical realization and validation remain challenging. In this project, we explore the Continuous k-Nearest Neighbors (CkNN) graph construction introduced by Berry and Sauer as a mathematically consistent framework for manifold reconstruction in the context of topological data analysis (TDA). We examine how CkNN circumvents the limitations of traditional $\epsilon$-ball and standard $k$NN methods by adapting to non-uniform sampling densities and enabling spectral convergence to the Laplace--Beltrami operator. This convergence allows for the consistent recovery of topological features---such as connected components and higher Betti numbers---in both compact and non-compact settings. Through this lens, we interpret CkNN as a concrete instantiation of the manifold hypothesis, grounding an intuitive idea in rigorous spectral and topological theory.
  \end{abstract}

  \tableofcontents

  \section{Introduction}

  The manifold hypothesis suggests that real-world data, despite residing in a high-dimensional ambient space, is concentrated near a lower-dimensional manifold. This idea is widespread in machine learning and data science, underpinning techniques such as dimensionality reduction, clustering, and representation learning. The assumption that high-dimensional data has low intrinsic dimension enables efficient algorithms and interpretable models, and it motivates the use of geometric and topological tools in data analysis.

  In topological data analysis (TDA), one often aims to extract structural information about a dataset--such as the number of clusters, loops, or voids--using only pairwise distances between data points. However, traditional methods such as persistent homology, based on Vietoris--Rips (VR) filtrations, are sensitive to scale and do not guarantee convergence to the topology of the underlying manifold.

  In this work, we focus on a concrete realization of the manifold hypothesis via the \emph{Continuous $k$-Nearest Neighbors} (CkNN) method introduced by Berry and Sauer~\cite{1606.02353}. This method enables consistent recovery of a manifold’s geometry and topology from point cloud data, offering a principled mathematical foundation for manifold learning in the context of TDA. Specifically, we explore how CkNN bridges the gap between abstract geometric assumptions and rigorous topological computation, and we interpret its spectral convergence results as a partial resolution of the manifold hypothesis in practice.

  \section{Preliminaries}

  We briefly recall some mathematical background relevant to our discussion. For a smooth $d$-dimensional manifold $\M$ embedded in $\R^D$, we assume $\M$ is equipped with a Riemannian metric $g$, inducing a natural volume form $dV$ and inner products on tangent spaces $T_p\M$. The Laplace--Beltrami operator $\Delta_\M$ is defined on smooth functions $f \in C^\infty(\M)$ as
  \[%
    \Delta_\M f = \div(\nabla f)
  ,\]%
  where $\nabla$ and $\div$ are the gradient and divergence defined with respect to the metric $g$.

  Let $\mu$ be a Borel probability measure supported on $\M$, with associated density $q \in C^2(\M)$ with respect to the volume form $dV$. A point cloud $X = \{x_i\}_{i=1}^n$ is modeled as an i.i.d. sample from $\mu$. For analysis of geometric and spectral convergence, it is common to define a kernel $K_\epsilon(x,y)$ and associated operators:
  \[%
    T_\epsilon f(x) = \frac{1}{\epsilon^{\sfrac{d}{2}}} \int_\M K\left(\frac{\|x - y\|^2}{\epsilon}\right) f(y) q(y)\,dV(y)
  ,\]%
  as used in manifold learning literature, e.g., in diffusion maps and Laplacian Eigenmaps~\cite{belkin_niyogi_2003}.

  Given a graph $G = (V,E)$ with unweighted adjacency matrix $A$, the (unnormalized) graph Laplacian is $L = D - A$, where $D$ is the degree matrix. If $G$ is constructed from a point cloud sampled from a manifold, then $L$ is said to converge to $\Delta_\M$ in a suitable limit if the spectrum and eigenfunctions of $L$ converge (in an appropriate sense) to those of $\Delta_\M$.

  \section{The Manifold Hypothesis}

  Let $\M \subset \R^D$ be a compact, smooth $d$-dimensional Riemannian manifold. The \emph{manifold hypothesis} posits that data sets $X = \{x_i\}_{i=1}^n \subset \R^D$ arising in real-world applications are sampled (possibly with noise) from a probability distribution $\mu$ supported on or near $\M$. More formally, we assume:
  \begin{itemize}
    \item $\M$ is a compact, connected, orientable Riemannian manifold without boundary;

    \item $\mu$ is a Borel probability measure on $\M$ with a smooth, strictly positive density $q$ with respect to the Riemannian volume form;

    \item $X$ is a finite i.i.d. sample from $\mu$.
  \end{itemize}

  The manifold hypothesis provides a geometric prior for high-dimensional data analysis: although the ambient dimension $D$ may be large, the intrinsic structure of the data lies in a space of much lower dimension $d \ll D$. This assumption enables both computational and statistical advantages, and underpins a variety of manifold learning techniques (e.g., Isomap, diffusion maps, Laplacian Eigenmaps) as well as topological data analysis pipelines.

  The central question is: given only $X$, can we recover information about the geometry and topology of $\M$? One approach is to estimate geometric quantities such as the Laplace--Beltrami operator $\Delta_\M$, or to compute topological invariants such as the Betti numbers $\beta_k(\M)$. However, directly testing the manifold hypothesis is statistically subtle. Fefferman, Mitter, and Narayanan~\cite{1310.0425} consider this problem in the setting of statistical hypothesis testing. They show that, under regularity assumptions on the reach and curvature of $\M$ and the noise distribution, there exist minimax optimal tests for whether a sample lies near a $d$-dimensional manifold. Crucially, they also establish lower bounds on the number of samples $n$ required for reliable recovery of topological features.

  In practice, consistent estimation of topological invariants from point clouds requires constructions that adapt to both the geometry of $\M$ and the sampling density $q$. As we will explore in later sections, the Continuous $k$-Nearest Neighbors (CkNN) graph~\cite{1606.02353} achieves this by producing a discrete Laplacian operator whose spectral properties converge to those of $\Delta_\M$, thereby linking the observed data directly to the latent manifold structure.

  \section{Topological Data Analysis and Persistent Homology}

  Topological Data Analysis (TDA) provides a framework for extracting structural information about a data set by leveraging techniques from algebraic topology. Given a point cloud $X = \{x_1, \dots, x_n\} \subset \R^D$, one seeks to understand its underlying topological features: connected components, loops, voids, and more generally, $k$-dimensional holes. A common approach is to construct a family of combinatorial complexes parameterized by a scale parameter and compute the homology groups of each.

  Let $\K_\epsilon$ be a simplicial complex constructed from $X$ at scale $\epsilon > 0$ -- typically via the Vietoris--Rips (VR) complex:
  \[%
    \VR(X, \epsilon) \coloneq \left\{ \sigma \subset X : \|x_i - x_j\| \leq \epsilon~\forall~x_i, x_j \in \sigma \right\}
  .\]%
  This yields a nested filtration:
  \[%
    \VR(X, \epsilon_1) \subseteq \VR(X, \epsilon_2) \subseteq \cdots \quad (\epsilon_1 < \epsilon_2 < \cdots)
  .\]%
  The $k$-th persistent homology is then computed via the induced maps on homology:
  \[%
    H_k(\VR(X, \epsilon_1)) \to H_k(\VR(X, \epsilon_2)) \to \cdots
  .\]%
  Each homology class $\alpha \in H_k(\VR(X, \epsilon_i))$ has a birth time $\epsilon_i$ and may die at $\epsilon_j$ if it maps to zero in $H_k(\VR(X, \epsilon_j))$ but not before. These intervals $[\epsilon_{\text{birth}}, \epsilon_{\text{death}})$ form the $k$-dimensional persistence diagram.

  However, while persistent homology provides a powerful multi-scale summary of topological features, it suffers from key theoretical limitations. In particular, the VR complex is sensitive to the choice of scale $\epsilon$, and there is no canonical scale at which one recovers the true topology of the underlying manifold. More importantly, as noted by Berry and Sauer~\cite{1606.02353}, the VR construction lacks a consistent limit: the persistent homology does not converge (in any strong sense) to the homology of the underlying manifold $\M$ as $n \to \infty$, unless certain weighted or density-adaptive methods are introduced.

  Let us denote by $\M$ a compact $d$-dimensional Riemannian manifold embedded in $\R^D$, and suppose $X_n = \{x_1, \dots, x_n\}$ is an i.i.d. sample from a distribution supported on or near $\M$. Then the fundamental question is whether
  \[%
    \lim_{n \to \infty} H_k(\VR(X_n, \epsilon_n)) \cong H_k(\M)
  ,\]%
  for some suitable sequence $\epsilon_n$. In general, this fails: no single scale $\epsilon$ can consistently recover all Betti numbers of $\M$ without either under- or over-connecting components depending on local sampling density. This phenomenon is exacerbated in nonuniformly sampled data sets, where fixed-$\epsilon$ constructions misrepresent both local and global topology (cf.~Fig.~1 of~\cite{1606.02353}).

  These issues highlight the need for consistent, density-adaptive constructions that preserve both geometric and topological information. The Continuous $k$-Nearest Neighbors (CkNN) graph, introduced by~\cite{1606.02353}, was designed precisely to resolve this inconsistency. It defines an unweighted graph that reflects a consistent geometry, with the remarkable property that the unnormalized graph Laplacian converges spectrally to the Laplace--Beltrami operator on $\M$. This spectral convergence is the gateway to topological consistency, connecting the discrete Laplacian to the continuous Laplace--de~Rham complex and ultimately to simplicial homology via spectral exterior calculus~\cite{1606.02353}.

  Thus, while traditional VR-based persistent homology captures topological features at multiple scales, it lacks a unifying, convergent structure. CkNN aims to provide this structure: a single graph construction that encodes the correct topology at once, free from the scale sensitivity inherent to conventional TDA pipelines.

  \section{The CkNN Framework}

  Let $X = \{x_1, \dots, x_n\} \subset \R^D$ be a finite sample drawn i.i.d. from a Borel probability measure $\mu$ supported on a compact $d$-dimensional Riemannian manifold $\M \subset \R^D$, with smooth density $q$ with respect to the volume form $dV$. The goal is to construct a graph $G_n$ on $X$ such that the unnormalized graph Laplacian $L_n$ converges, in
  a spectral sense, to the Laplace--Beltrami operator $\Delta_\M$ on $\M$.

  The Continuous $k$-Nearest Neighbors (CkNN) method, introduced in~\cite{1606.02353}, defines an unweighted graph whose edge set is determined by both pairwise distances and local sampling density. For each point $x_i \in X$, let $r_k(x_i)$ denote the Euclidean distance to its $k$th nearest neighbor. Define the CkNN graph $G_n = (X, E_n)$ by placing an undirected edge between $x_i$ and $x_j$ if and only if
  \[%
    \|x_i - x_j\| < \delta \sqrt{r_k(x_i) r_k(x_j)}
  ,\]%
  for a fixed parameter $\delta > 0$.

  This construction has two key features:
  \begin{itemize}
    \item It adapts to local sampling density via $r_k(x)$, effectively normalizing distances to avoid over- or under-connecting in high- or low-density regions.

    \item It produces an unweighted adjacency matrix $A_n$ whose associated Laplacian $L_n = D_n - A_n$ (where $D_n$ is the diagonal degree matrix) retains topological structure in the limit $n \to \infty$.
  \end{itemize}

  Berry and Sauer prove that, under mild regularity conditions, $L_n$ converges spectrally to $\Delta_\M$ up to a multiplicative constant. More precisely:

  \begin{theorem}[Spectral Convergence of CkNN~\cite{1606.02353}]
    Let $f_1, \dots, f_m$ be the first $m$ eigenfunctions of the unnormalized graph Laplacian $L_n$ on the CkNN graph $G_n$, with corresponding eigenvalues $\lambda_1^{(n)} \leq \cdots \leq \lambda_m^{(n)}$. Then, with high probability as $n \to \infty$,
    \[%
      \lambda_j^{(n)} \to C \cdot \lambda_j \aand f_j^{(n)} \to f_j
    ,\]%
    for $j = 1, \dots, m$, where $\lambda_j$ and $f_j$ are the eigenvalues and eigenfunctions of the Laplace--Beltrami operator $\Delta_\M$, and $C > 0$ is a constant depending on $\delta$, $k$, and the density $q$.
  \end{theorem}

  In particular, this means that CkNN recovers not only the spectrum of $\Delta_\M$ (up to scaling), but also its eigenfunctions --- which encode the intrinsic geometry of the manifold. Since the Laplace--Beltrami operator plays a central role in spectral geometry and in the Hodge theory underlying topological invariants, this result opens a rigorous path from point-cloud data to manifold structure.

  Furthermore, because the CkNN graph is unweighted, it can be used to build simplicial complexes (e.g., clique complexes) for TDA. This enables one to compute Betti numbers using persistent homology, while being guaranteed (in principle) that the resulting homological features are consistent in the large-sample limit.

  Compared to $\epsilon$-ball graphs and traditional $k$NN graphs, CkNN enjoys consistency under nonuniform sampling. Fixed-$\epsilon$ graphs fail to adapt to changing point density and lead to topological distortions, while standard $k$NN graphs are typically weighted and thus incompatible with standard persistent homology software. CkNN provides a principled solution to both problems.

  Finally, the parameter $\delta$ provides a natural scale control. For appropriate choices of $\delta$, the CkNN graph captures the topological regime of the manifold, and Berry and Sauer propose heuristic procedures for tuning $\delta$ in practice (see~\cite{1606.02353}, \S 6).

  Thus, CkNN provides a consistent and geometrically faithful way to bridge discrete data and continuous manifold structure. In the next section, we interpret this as a partial realization of the manifold hypothesis.

  \section{Manifold Reconstruction and the Hypothesis}

  Let $\M \subset \R^D$ be a compact, connected, smooth $d$-dimensional Riemannian manifold without boundary, and let $\mu$ be a Borel probability measure on $\M$ with smooth, strictly positive density $q$ with respect to the Riemannian volume form $dV$. The manifold hypothesis posits that a data set $X = \{x_1, \dots, x_n\} \subset \R^D$ is sampled i.i.d. from $\mu$, and that $\M$ encodes the intrinsic geometric and topological structure of the data.

  The central task of manifold reconstruction is to infer geometric and topological invariants of $\M$ from the finite sample $X$. Specifically, one seeks to recover:

  \begin{itemize}
    \item The geometry of $\M$, as described by the Riemannian metric $g$, and more tractably, the Laplace--Beltrami operator $\Delta_\M$;

    \item The topology of $\M$, as given by homological invariants such as the Betti numbers $\beta_k(\M)$;

    \item A discrete structure (e.g., a graph or simplicial complex) on $X$ that approximates $\M$ in a consistent way.
  \end{itemize}

  We interpret the CkNN method as providing a statistically consistent estimator of both the differential and topological structure of $\M$, thereby offering a constructive realization of the manifold hypothesis.

  \subsection[From Graph Laplacians to $\Delta_M$]{From Graph Laplacians to $\Delta_\M$}

  Let $G_n = (X, E_n)$ be the CkNN graph with adjacency matrix $A_n$ and unnormalized graph Laplacian $L_n = D_n - A_n$. Define the function space $\F_n = \{f : X \to \R\}$, and equip it with the discrete inner product
  \[%
    \langle f, g \rangle_n = \frac{1}{n} \sum_{i=1}^n f(x_i) g(x_i)
  .\]%
  The Laplacian $L_n$ defines a symmetric operator on $\F_n$, and its spectrum $\{ \lambda_j^{(n)} \}$ and eigenvectors $\{ f_j^{(n)} \}$ are discrete analogues of the eigenvalues and eigenfunctions of $\Delta_\M$.

  Berry and Sauer~\cite{1606.02353} show that, under mild assumptions, $L_n$ converges spectrally to $C \cdot \Delta_\M$ as $n \to \infty$, for some constant $C > 0$. That is, for each fixed $j \in \mathbb{N}$,
  \[%
    \lambda_j^{(n)} \to C \cdot \lambda_j \aand f_j^{(n)} \to f_j \in L^2(\M, q\, dV)
  ,\]%
  where $\lambda_j$ and $f_j$ are the $j$-th eigenvalue and eigenfunction of $\Delta_\M$ acting on $L^2(\M, q\, dV)$. This is convergence in the sense of operator spectra (specifically, norm-resolvent or strong resolvent convergence), and it implies that $L_n$ preserves key geometric information about $\M$.

  \subsection{Spectral Geometry and Topology}

  The Laplace--Beltrami operator $\Delta_\M$ encodes more than just geometry. Via Hodge theory, its eigenstructure governs the de Rham cohomology of $\M$. In particular:
  \begin{itemize}
    \item The dimension of the kernel of the Hodge Laplacian on $k$-forms, $\ker \Delta_k$, is isomorphic to the $k$-th Betti number $\beta_k(\M)$.

    \item For $k = 0$, $\ker \Delta_0$ corresponds to the space of constant functions, and its dimension equals the number of connected components of $\M$.
  \end{itemize}

  The unnormalized graph Laplacian $L_n$ acts on functions (0-forms) and provides an estimator for $\Delta_0$. More generally, using tools from spectral exterior calculus (SEC), one can define discrete Hodge Laplacians $\Delta_k^{(n)}$ on combinatorial $k$-forms, and under suitable constructions (e.g., clique complexes from CkNN graphs), these discrete operators converge to their continuous counterparts $\Delta_k$.

  Thus, via spectral convergence, CkNN enables consistent estimation of the Betti numbers $\beta_k(\M)$ from finite samples:
  \[%
    \beta_k(\M) = \dim \ker \Delta_k = \lim_{n \to \infty} \dim \ker \Delta_k^{(n)}
  .\]%
  This bridges geometric learning and topological data analysis through a shared spectral framework.

  \subsection{Consistency and Realization of the Hypothesis}

  We can now make precise what it means for CkNN to realize the manifold hypothesis.

  Let $X_n = \{x_1, \dots, x_n\}$ be an i.i.d. sample from $\mu$, and let $L_n$ be the unnormalized graph Laplacian of the CkNN graph $G_n$. Suppose we define $H_k^{(n)}$ to be the $k$-th persistent homology of a simplicial complex (e.g., the clique complex) built from $G_n$.

  Under the assumptions of Berry and Sauer, we have the convergence:
  \[%
    \lim_{n \to \infty} H_k^{(n)} \cong H_k(\M)
  ,\]%
  provided that the parameter $\delta$ is chosen in the topological regime (i.e., $\delta$ lies in an interval where the CkNN graph captures the correct topological structure). This defines topological consistency, a property rarely achieved by fixed-radius or standard $k$NN graphs.

  Moreover, since CkNN yields a single graph that preserves all homology generators simultaneously (unlike persistence which tracks generators across scales), it offers a strong practical interpretation of the manifold hypothesis: the existence of a finite sample $X_n$ from which one can recover all topological features of $\M$ via a consistent discrete structure.

  Thus, the CkNN method provides a partial realization of the manifold hypothesis in the following sense:

  \begin{itemize}
    \item Given: i.i.d. samples from a smooth measure on a compact manifold,

    \item Construct: a CkNN graph with appropriate parameters,

    \item Recover: the correct topological invariants and the spectrum of $\Delta_\M$,

    \item Conclude: consistency with the manifold hypothesis.
  \end{itemize}

  This not only affirms the relevance of the hypothesis in practical data analysis but also offers a concrete pipeline for its operationalization in topological and spectral learning.

  \section{Broader Context and Related Work}

  The reconstruction of geometric and topological structure from point cloud data has been a central pursuit in manifold learning, spectral geometry, and topological data analysis (TDA). The Continuous $k$-Nearest Neighbors (CkNN) method sits at the intersection of these fields, resolving critical limitations of earlier approaches through a blend of geometric adaptivity and spectral consistency.

  This section situates CkNN within the broader landscape of data-driven manifold reconstruction and TDA, emphasizing both the theoretical advances and practical advantages over prior methods.

  \subsection{Spectral Methods in Manifold Learning}

  Early manifold learning methods, such as Laplacian Eigenmaps~\cite{belkin_niyogi_2003} and Diffusion Maps~\cite{coifman_lafon_2006}, approximate the geometry of a manifold $\M \subset \R^D$ by constructing a weighted graph $G_n$ on a point cloud $X = \{x_1, \dots, x_n\} \subset \R^D$, typically via a fixed-bandwidth kernel:
  \[%
    K_\epsilon(x_i, x_j) = \exp\left( -\frac{\|x_i - x_j\|^2}{\epsilon} \right)
  ,\]%
  with edge weights $w_{ij} = K_\epsilon(x_i, x_j)$, and graph Laplacian
  \[%
    L_n = D_n - W_n
  ,\]%
  where $W_n = (w_{ij})$ and $D_n$ is the diagonal degree matrix. These constructions yield convergence:
  \[%
    L_n \to \Delta_\M \quad \text{(up to normalization)}
  ,\]%
  under strong regularity assumptions on the sampling distribution $\mu$ and manifold $\M$. However, these graphs are weighted, and thus not directly compatible with persistent homology, which typically assumes unweighted complexes (e.g., Vietoris--Rips or clique complexes built from adjacency graphs).

  Moreover, the bandwidth parameter $\epsilon$ must shrink with $n$ at a precise rate to ensure consistency (cf.~Belkin and Niyogi, 2007). The choice of $\epsilon$ affects both spectral accuracy and graph connectivity, leading to a tension between geometric fidelity and topological interpretability.

  In contrast, the CkNN method bypasses these limitations:

  \begin{itemize}
    \item It constructs unweighted graphs using a density-adaptive local scale derived from $k$-NN distances.

    \item It achieves spectral convergence to $\Delta_\M$ with fixed $k$ and fixed $\delta$, under mild assumptions.

    \item It enables the construction of simplicial complexes compatible with persistent homology, while maintaining geometric consistency.
  \end{itemize}

  Thus, CkNN unifies the spectral convergence guarantees of Laplacian-based methods with the combinatorial tractability required for TDA.

  \subsection{Limitations of Traditional TDA Pipelines}

  Topological Data Analysis has traditionally relied on scale-parameterized
  constructions such as the Vietoris--Rips complex $\VR(X, \epsilon)$, \v{C}ech
  complex, or witness complexes. These complexes yield a filtration:
  \[%
    \K_{\epsilon_1} \subseteq \K_{\epsilon_2} \subseteq \dots
  ,\]%
  and homology is computed over each $\K_{\epsilon_i}$, producing persistent homology and corresponding persistence diagrams or barcodes.

  The appeal of persistent homology lies in its robustness to noise and its multiscale character. However, the following theoretical limitations apply:

  \begin{enumerate}
    \item Lack of a consistent limit: As shown by Berry and Sauer~\cite{1606.02353}, persistent homology computed from $\VR(X_n, \epsilon_n)$ does not, in general, converge to $H_k(\M)$ as $n \to \infty$.

    \item Scale sensitivity: No single $\epsilon$ recovers all Betti numbers reliably, especially under non-uniform sampling.

    \item Incompatibility with spectral convergence: The $\VR$ complex lacks a spectral operator whose eigenstructure approximates $\Delta_\M$.
  \end{enumerate}

  While some extensions, such as density-sensitive filtrations or weighted complexes, attempt to address these issues, they introduce significant algorithmic and theoretical complexity, and often rely on heuristics or lack convergence guarantees.

  CkNN resolves these challenges by producing a single unweighted graph whose spectrum converges to that of $\Delta_\M$, thereby enabling a topologically consistent complex and a geometrically faithful Laplacian simultaneously.

  \subsection{Statistical Testing and the Manifold Hypothesis}

  Fefferman, Mitter, and Narayanan~\cite{1310.0425} formulate the manifold hypothesis as a statistical testing problem: given i.i.d. samples from a distribution $P$ on a Hilbert space $H$, determine whether there exists a $d$-dimensional submanifold $\M$ with bounded volume and positive reach such that the mean squared distance
  \[%
    L(\M, P) = \int_H \operatorname{dist}(x, \M)^2 \, dP(x)
  ,\]%
  is small.

  They prove minimax lower bounds for this problem, showing that the number of samples required for consistent hypothesis testing grows at least as
  \[%
    n \gtrsim \left( \frac{1}{\tau^d \epsilon^{d/2}} + \frac{1}{\tau^d} \right)
  ,\]%
  where $\tau$ is the reach of the manifold and $\epsilon$ is the desired approximation error. These results imply that manifold learning is fundamentally sample-complex, especially in high intrinsic dimension or under small reach.

  While their approach yields provable guarantees on manifold detection, it does not offer an explicit reconstruction method. In contrast, CkNN provides an algorithmic construction of the manifold's topology and geometry from finite samples, sidestepping the need for hypothesis testing in favor of constructive recovery.

  Thus, CkNN complements the Fefferman–Mitter–Narayanan framework: the latter offers impossibility results and sample complexity bounds; the former supplies an implementable estimator consistent with the same underlying assumptions.

  \subsection{Spectral Exterior Calculus and Higher Homology}

  Recent work in spectral exterior calculus (SEC)~\cite{1802.01209} aims to extend Laplacian-based techniques to higher-degree forms, thereby enabling approximation of the Hodge Laplacians $\Delta_k$ acting on differential $k$-forms. These operators govern the de Rham cohomology of $\M$, and their kernel dimensions yield the higher Betti numbers:
  \[%
    \beta_k(\M) = \dim \ker \Delta_k
  .\]%

  While traditional TDA computes homology through combinatorial boundary maps, SEC provides an analytic pathway using function spaces and spectral operators. CkNN provides the discretization foundation for SEC by supplying consistent approximations of $\Delta_0$ and (conjecturally) $\Delta_k$ via discrete exterior operators constructed from the unweighted CkNN graph.

  Hence, CkNN can be viewed as the discrete substrate upon which analytic methods like SEC are built, further bridging the gap between combinatorial and spectral topology.

  \section{Conclusion}

  In this paper, we have explored how the manifold hypothesis, a foundational assumption in geometric learning, finds a rigorous and topologically meaningful realization in the CkNN framework. CkNN enables consistent manifold reconstruction by linking discrete data graphs to smooth differential operators, providing a spectral path from samples to structure.

  This work highlights the value of integrating manifold-based assumptions into topological pipelines. Future directions include further development of spectral exterior calculus, consistency guarantees for higher-dimensional homology, and practical algorithms for parameter selection in CkNN.

  \begin{appendices}
    \section{Proof of Theorem}
  \end{appendices}

  \bibliographystyle{plainnat}
  \bibliography{references}

  \nocite*{}
\end{document}
